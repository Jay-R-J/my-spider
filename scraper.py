#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»ˆææ™ºèƒ½çˆ¬è™« - ä¸ªäººç™¾åº¦èœ˜è››ï¼ˆHTMLé‚®ä»¶ç¾åŒ–ç‰ˆï¼‰
åŠŸèƒ½ï¼š
- ä»ç§å­ç½‘å€å¼€å§‹ï¼Œè‡ªåŠ¨æå–é“¾æ¥å¹¶æ‰©æ•£
- éµå®ˆ robots.txt
- é™åˆ¶åŸŸåèŒƒå›´
- æ™ºèƒ½æå–æ­£æ–‡ã€ç»“æ„åŒ–æ•°æ®ã€Open Graph å…ƒæ•°æ®
- é€šè¿‡ç¾è§‚çš„ HTML é‚®ä»¶å‘é€è¯¦ç»†æŠ¥å‘Šï¼ˆé“¾æ¥å¯ç‚¹å‡»ï¼‰
- çŠ¶æ€æŒä¹…åŒ–ï¼Œæ¯æ¬¡è¿è¡Œæ¥ç€ä¸Šæ¬¡
"""

import requests
from bs4 import BeautifulSoup
import os
import time
import logging
import smtplib
from email.mime.text import MIMEText
from urllib.parse import urljoin, urlparse
from urllib.robotparser import RobotFileParser
import re
from datetime import datetime
from typing import Set, List, Dict, Any, Optional

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥ï¼ˆå¸¦é”™è¯¯æç¤ºï¼‰
try:
    import trafilatura
except ImportError:
    raise ImportError("è¯·å®‰è£… trafilatura: pip install trafilatura")

try:
    from readability import Document
except ImportError:
    raise ImportError("è¯·å®‰è£… readability-lxml: pip install readability-lxml")

try:
    import extruct
except ImportError:
    raise ImportError("è¯·å®‰è£… extruct: pip install extruct")

# ==================== é…ç½®åŒºåŸŸ ====================
# æ¯æ¬¡è¿è¡Œæœ€å¤šæŠ“å–å¤šå°‘é¡µï¼ˆå»ºè®® 10-20ï¼Œé¿å…è¶…æ—¶å’Œå° IPï¼‰
MAX_PAGES_PER_RUN: int = 15
# è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰ï¼Œé™ä½æœåŠ¡å™¨å‹åŠ›
REQUEST_DELAY: int = 2
# ç”¨æˆ·ä»£ç†ï¼Œè¡¨æ˜èº«ä»½
USER_AGENT: str = "Mozilla/5.0 (compatible; MyPersonalSpider/1.0; +https://github.com/Jay-R-J/my-spider)"
# å…è®¸æŠ“å–çš„ä¼˜è´¨å†…å®¹åŸŸåç»„åˆ
ALLOWED_DOMAINS: List[str] = [
    "baike.baidu.com",      # ç™¾åº¦ç™¾ç§‘ - ç»“æ„åŒ–çŸ¥è¯†
    "zhidao.baidu.com",     # ç™¾åº¦çŸ¥é“ - é—®ç­”
    "tieba.baidu.com",      # ç™¾åº¦è´´å§ - è®ºå›è®¨è®º
    "github.com",           # GitHub - å¼€æºé¡¹ç›®
    "stackoverflow.com",    # Stack Overflow - ç¼–ç¨‹é—®ç­”
    "wikipedia.org",        # ç»´åŸºç™¾ç§‘ - å¤šè¯­è¨€ç™¾ç§‘
    "juejin.cn",            # æ˜é‡‘ - æŠ€æœ¯æ–‡ç« 
]
# ç§å­æ–‡ä»¶è·¯å¾„
SEEDS_FILE: str = "seeds.txt"
# å¾…æŠ“å–é˜Ÿåˆ—æ–‡ä»¶
PENDING_FILE: str = "pending.txt"
# å·²è®¿é—®è®°å½•æ–‡ä»¶
VISITED_FILE: str = "visited.txt"
# æ•°æ®ä¿å­˜ç›®å½•
DATA_DIR: str = "data"
# æ—¥å¿—çº§åˆ«
LOG_LEVEL: int = logging.INFO
# é‚®ä»¶æ­£æ–‡ä¸­æ¯ä¸ªé¡µé¢çš„æ­£æ–‡é¢„è§ˆæœ€å¤§é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
PREVIEW_MAX_LENGTH: int = 2000
# ==================== é‚®ä»¶é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰====================
MAIL_USER: Optional[str] = os.environ.get("MAIL_USER")
MAIL_PASS: Optional[str] = os.environ.get("MAIL_PASS")
MAIL_TO: Optional[str] = os.environ.get("MAIL_TO")
# ===============================================================

# é…ç½®æ—¥å¿—
logging.basicConfig(level=LOG_LEVEL, format='%(asctime)s - %(levelname)s - %(message)s')

# å…¨å±€ robot è§£æå™¨ç¼“å­˜
robot_parsers: Dict[str, Optional[RobotFileParser]] = {}

def get_robots_parser(domain: str) -> Optional[RobotFileParser]:
    """è·å– domain çš„ robots è§£æå™¨ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    if domain in robot_parsers:
        return robot_parsers[domain]
    parser = RobotFileParser()
    parser.set_url(f"https://{domain}/robots.txt")
    try:
        parser.read()
        robot_parsers[domain] = parser
    except Exception as e:
        logging.warning(f"è¯»å– {domain}/robots.txt å¤±è´¥: {e}")
        robot_parsers[domain] = None
    return robot_parsers[domain]

def can_fetch(url: str, user_agent: str = USER_AGENT) -> bool:
    """æ£€æŸ¥ robots.txt æ˜¯å¦å…è®¸æŠ“å–è¯¥ URL"""
    domain = urlparse(url).netloc
    parser = get_robots_parser(domain)
    if parser is None:
        # æ— æ³•è·å– robotsï¼Œé»˜è®¤å…è®¸ï¼ˆè°¨æ…ï¼‰
        return True
    return parser.can_fetch(user_agent, url)

def load_set(filename: str) -> Set[str]:
    """ä»æ–‡ä»¶åŠ è½½é›†åˆï¼Œæ¯è¡Œä¸€ä¸ªå…ƒç´ """
    if not os.path.exists(filename):
        return set()
    with open(filename, 'r', encoding='utf-8') as f:
        return {line.strip() for line in f if line.strip()}

def save_set(filename: str, data_set: Set[str]) -> None:
    """ä¿å­˜é›†åˆåˆ°æ–‡ä»¶"""
    with open(filename, 'w', encoding='utf-8') as f:
        for item in sorted(data_set):
            f.write(item + '\n')

def load_list(filename: str) -> List[str]:
    """ä»æ–‡ä»¶åŠ è½½åˆ—è¡¨ï¼ˆä¿ç•™é¡ºåºï¼‰"""
    if not os.path.exists(filename):
        return []
    with open(filename, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f if line.strip()]

def save_list(filename: str, data_list: List[str]) -> None:
    """ä¿å­˜åˆ—è¡¨åˆ°æ–‡ä»¶"""
    with open(filename, 'w', encoding='utf-8') as f:
        for item in data_list:
            f.write(item + '\n')

def is_allowed_domain(url: str) -> bool:
    """æ£€æŸ¥åŸŸåæ˜¯å¦åœ¨ ALLOWED_DOMAINS ä¸­"""
    if not ALLOWED_DOMAINS:
        return True
    domain = urlparse(url).netloc
    for allowed in ALLOWED_DOMAINS:
        if domain == allowed or domain.endswith('.' + allowed):
            return True
    return False

def normalize_url(url: str) -> str:
    """æ ‡å‡†åŒ– URLï¼šå»é™¤ fragmentï¼Œä¿ç•™ scheme+netloc+path"""
    parsed = urlparse(url)
    return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"

def extract_links(soup: BeautifulSoup, base_url: str) -> List[str]:
    """ä» BeautifulSoup å¯¹è±¡ä¸­æå–æ‰€æœ‰é“¾æ¥ï¼Œè¿”å›æ ‡å‡†åŒ–åçš„ URL åˆ—è¡¨"""
    links = []
    for a in soup.find_all('a', href=True):
        href = a['href'].strip()
        if not href:
            continue
        absolute = urljoin(base_url, href)
        if absolute.startswith(('http://', 'https://')):
            normalized = normalize_url(absolute)
            links.append(normalized)
    return links

def extract_structured_data(html: str, url: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨ extruct æå– JSON-LDã€å¾®æ•°æ®ç­‰ç»“æ„åŒ–æ•°æ®ã€‚
    è¿”å›åŒ…å«å…³é”®ä¿¡æ¯çš„å­—å…¸ã€‚
    """
    try:
        data = extruct.extract(html, url, uniform=True)
        summary: Dict[str, Any] = {}

        if data.get('json-ld'):
            for item in data['json-ld']:
                if isinstance(item, dict):
                    if 'name' in item:
                        summary['name'] = item['name']
                    if 'description' in item and 'description' not in summary:
                        summary['description'] = item['description']
                    if 'offers' in item:
                        offers = item['offers']
                        if isinstance(offers, dict):
                            price = offers.get('price')
                            if price:
                                summary['price'] = price
                            currency = offers.get('priceCurrency')
                            if currency:
                                summary['currency'] = currency
                        elif isinstance(offers, list) and offers:
                            first_offer = offers[0]
                            if isinstance(first_offer, dict):
                                price = first_offer.get('price')
                                if price:
                                    summary['price'] = price
                                currency = first_offer.get('priceCurrency')
                                if currency:
                                    summary['currency'] = currency
                    if 'aggregateRating' in item:
                        rating = item['aggregateRating'].get('ratingValue')
                        if rating:
                            summary['rating'] = rating
                    if 'reviewCount' in item:
                        summary['review_count'] = item['reviewCount']

        if data.get('microdata'):
            for item in data['microdata']:
                props = item.get('properties', {})
                if 'price' in props:
                    summary['price'] = props['price']
                if 'name' in props and 'name' not in summary:
                    summary['name'] = props['name']

        return summary
    except Exception as e:
        logging.warning(f"ç»“æ„åŒ–æ•°æ®æå–å¤±è´¥: {e}")
        return {}

def extract_opengraph(soup: BeautifulSoup) -> Dict[str, str]:
    """æå– Open Graph å…ƒæ•°æ®"""
    og = {}
    for meta in soup.find_all('meta', property=re.compile(r'^og:')):
        prop = meta.get('property')
        content = meta.get('content')
        if prop and content:
            key = prop[3:]  # å»æ‰ 'og:'
            og[key] = content
    return og

def extract_page_data(html: str, url: str) -> Dict[str, Any]:
    """
    ç»¼åˆæå–é¡µé¢çš„æ‰€æœ‰é‡è¦æ•°æ®ã€‚
    """
    data: Dict[str, Any] = {
        'url': url,
        'title': '',
        'meta_description': '',
        'og': {},
        'structured': {},
        'main_text': '',
        'extraction_method': 'none'
    }

    soup = BeautifulSoup(html, 'lxml')

    # æ ‡é¢˜
    if soup.title and soup.title.string:
        data['title'] = soup.title.string.strip()

    # Meta description
    meta_desc = soup.find('meta', attrs={'name': 'description'}) or soup.find('meta', attrs={'property': 'og:description'})
    if meta_desc and meta_desc.get('content'):
        data['meta_description'] = meta_desc['content'].strip()

    # Open Graph
    data['og'] = extract_opengraph(soup)

    # ç»“æ„åŒ–æ•°æ®
    data['structured'] = extract_structured_data(html, url)

    # ä¸»è¦æ–‡æœ¬æå–ï¼ˆä¼˜å…ˆç”¨ trafilaturaï¼‰
    extracted = trafilatura.extract(html, url=url, include_comments=False, include_tables=True, include_images=False)
    if extracted:
        data['main_text'] = extracted[:PREVIEW_MAX_LENGTH] + ('...' if len(extracted) > PREVIEW_MAX_LENGTH else '')
        data['extraction_method'] = 'trafilatura'
    else:
        # å›é€€åˆ° readability
        doc = Document(html)
        data['main_text'] = doc.summary()
        text_soup = BeautifulSoup(data['main_text'], 'lxml')
        data['main_text'] = text_soup.get_text(separator='\n', strip=True)[:PREVIEW_MAX_LENGTH]
        data['extraction_method'] = 'readability'
        if not data['main_text']:
            # æœ€åå¤‡é€‰ï¼šç®€å•å»æ ‡ç­¾å–æ–‡æœ¬
            for script in soup(["script", "style"]):
                script.decompose()
            text = soup.get_text(separator=' ', strip=True)
            text = re.sub(r'\s+', ' ', text)
            data['main_text'] = text[:PREVIEW_MAX_LENGTH]
            data['extraction_method'] = 'fallback'

    return data

def send_html_email(subject: str, body_html: str) -> None:
    """å‘é€ HTML æ ¼å¼é‚®ä»¶"""
    if not (MAIL_USER and MAIL_PASS and MAIL_TO):
        logging.warning("é‚®ä»¶é…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡å‘é€")
        return

    msg = MIMEText(body_html, 'html', 'utf-8')
    msg['Subject'] = subject
    msg['From'] = MAIL_USER
    msg['To'] = MAIL_TO

    try:
        server = smtplib.SMTP_SSL('smtp.qq.com', 465)
        server.login(MAIL_USER, MAIL_PASS)
        server.send_message(msg)
        server.quit()
        logging.info("HTMLé‚®ä»¶å‘é€æˆåŠŸ")
    except Exception as e:
        logging.error(f"é‚®ä»¶å‘é€å¤±è´¥: {e}")

def generate_html_report(
    pages_crawled: int,
    new_links_found: int,
    failed_urls: List[str],
    unique_pending: List[str],
    visited_count: int,
    page_details: List[Dict]
) -> str:
    """ç”Ÿæˆç¾è§‚çš„ HTML æŠ¥å‘Š"""
    html = []
    html.append('''<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            background-color: white;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            padding: 25px;
        }
        .header {
            text-align: center;
            padding-bottom: 20px;
            border-bottom: 2px solid #4CAF50;
            margin-bottom: 20px;
        }
        .header h1 {
            color: #2c3e50;
            margin: 0;
            font-size: 28px;
        }
        .header p {
            color: #7f8c8d;
            margin: 5px 0 0;
        }
        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));
            gap: 15px;
            margin: 25px 0;
        }
        .stat-card {
            background: #f8f9fa;
            border-radius: 8px;
            padding: 15px;
            text-align: center;
            box-shadow: 0 2px 5px rgba(0,0,0,0.05);
        }
        .stat-card .value {
            font-size: 32px;
            font-weight: bold;
            color: #2c3e50;
        }
        .stat-card .label {
            font-size: 14px;
            color: #7f8c8d;
            margin-top: 5px;
        }
        .page-card {
            background: white;
            border: 1px solid #e9ecef;
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.05);
            transition: transform 0.2s;
        }
        .page-card:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        }
        .page-title {
            font-size: 20px;
            font-weight: 600;
            color: #2c3e50;
            margin: 0 0 10px 0;
        }
        .page-title a {
            color: #3498db;
            text-decoration: none;
        }
        .page-title a:hover {
            text-decoration: underline;
        }
        .page-url {
            font-size: 14px;
            color: #7f8c8d;
            word-break: break-all;
            margin-bottom: 15px;
            background: #f8f9fa;
            padding: 8px 12px;
            border-radius: 5px;
            border-left: 3px solid #3498db;
        }
        .page-url a {
            color: #3498db;
            text-decoration: none;
        }
        .page-url a:hover {
            text-decoration: underline;
        }
        .summary-box {
            background: #f1f9fe;
            border-left: 4px solid #3498db;
            padding: 15px;
            border-radius: 5px;
            margin: 15px 0;
            font-size: 15px;
            color: #2c3e50;
        }
        .meta-tags {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin: 15px 0;
        }
        .meta-tag {
            background: #e9ecef;
            color: #495057;
            padding: 4px 12px;
            border-radius: 20px;
            font-size: 13px;
            font-weight: 500;
        }
        .meta-tag.blue {
            background: #d4edda;
            color: #155724;
        }
        .meta-tag.green {
            background: #d1ecf1;
            color: #0c5460;
        }
        .meta-tag.orange {
            background: #fff3cd;
            color: #856404;
        }
        .structured-data {
            background: #f8f9fa;
            border-radius: 8px;
            padding: 12px;
            margin-top: 15px;
            border: 1px dashed #adb5bd;
        }
        .structured-data h4 {
            margin: 0 0 8px 0;
            font-size: 16px;
            color: #2c3e50;
        }
        .structured-data pre {
            margin: 0;
            font-family: 'Courier New', monospace;
            font-size: 13px;
            color: #2c3e50;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        hr {
            border: 0;
            border-top: 1px solid #e9ecef;
            margin: 30px 0;
        }
        .footer {
            text-align: center;
            color: #95a5a6;
            font-size: 13px;
            margin-top: 30px;
            padding-top: 20px;
            border-top: 1px solid #e9ecef;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ•·ï¸ ç»ˆææ™ºèƒ½çˆ¬è™«è¿è¡ŒæŠ¥å‘Š</h1>
            <p>''' + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + '''</p>
        </div>
''')

    # ç»Ÿè®¡å¡ç‰‡
    html.append('<div class="stats-grid">')
    stats = [
        ('ğŸ“„ æŠ“å–é¡µé¢', pages_crawled),
        ('ğŸ”— æ–°å‘ç°é“¾æ¥', new_links_found),
        ('âŒ å¤±è´¥é“¾æ¥', len(failed_urls)),
        ('â³ å¾…æŠ“å–é˜Ÿåˆ—', len(unique_pending)),
        ('ğŸ“š ç´¯è®¡å·²æŠ“å–', visited_count)
    ]
    for label, value in stats:
        html.append(f'''
        <div class="stat-card">
            <div class="value">{value}</div>
            <div class="label">{label}</div>
        </div>
        ''')
    html.append('</div>')

    if not page_details:
        html.append('<p style="text-align:center;color:#7f8c8d;">æœ¬æ¬¡è¿è¡ŒæœªæŠ“å–åˆ°æ–°é¡µé¢ã€‚</p>')
    else:
        html.append(f'<h2 style="color:#2c3e50;">ğŸ“Œ æœ¬æ¬¡æŠ“å–è¯¦æƒ…ï¼ˆå…± {len(page_details)} é¡µï¼‰</h2>')

        for idx, data in enumerate(page_details, 1):
            html.append(f'<div class="page-card">')
            html.append(f'<div class="page-title">ğŸ“Œ {idx}. {data["title"]}</div>')
            html.append(f'<div class="page-url">ğŸ”— <a href="{data["url"]}">{data["url"]}</a></div>')

            # å†…å®¹æ‘˜è¦
            if data['main_text']:
                html.append(f'<div class="summary-box"><strong>ğŸ“ å†…å®¹æ‘˜è¦ï¼š</strong><br>{data["main_text"]}</div>')
            else:
                html.append('<div class="summary-box"><strong>ğŸ“ å†…å®¹æ‘˜è¦ï¼š</strong> æ— </div>')

            # å…ƒæ•°æ®æ ‡ç­¾
            meta_tags = []
            if data['meta_description']:
                meta_tags.append(f'<span class="meta-tag blue">ğŸ“„ æè¿°ï¼š{data["meta_description"][:100]}â€¦</span>')

            if data['og']:
                og_title = data['og'].get('title', '')
                if og_title and og_title != data['title']:
                    meta_tags.append(f'<span class="meta-tag green">ğŸ”— OGæ ‡é¢˜ï¼š{og_title[:50]}â€¦</span>')
                og_desc = data['og'].get('description', '')
                if og_desc:
                    meta_tags.append(f'<span class="meta-tag green">ğŸ“‹ OGæè¿°ï¼š{og_desc[:50]}â€¦</span>')

            if data['structured']:
                struct = data['structured']
                items = []
                if 'name' in struct:
                    items.append(f"äº§å“å:{struct['name']}")
                if 'price' in struct:
                    price = struct['price']
                    if 'currency' in struct:
                        price += f" {struct['currency']}"
                    items.append(f"ğŸ’° {price}")
                if 'rating' in struct:
                    items.append(f"â­ {struct['rating']}")
                if 'review_count' in struct:
                    items.append(f"ğŸ—£ï¸ {struct['review_count']}è¯„è®º")
                if items:
                    meta_tags.append('<span class="meta-tag orange">ğŸ“Š ç»“æ„åŒ–æ•°æ®ï¼š' + ' | '.join(items) + '</span>')

            if meta_tags:
                html.append('<div class="meta-tags">' + ''.join(meta_tags) + '</div>')

            # å®Œæ•´ç»“æ„åŒ–æ•°æ®æ˜¾ç¤ºï¼ˆå¯é€‰ï¼‰
            if data['structured'] and len(data['structured']) > 3:  # å¦‚æœç»“æ„åŒ–æ•°æ®ä¸°å¯Œï¼Œé¢å¤–å±•ç¤º
                html.append('<div class="structured-data">')
                html.append('<h4>ğŸ“‹ è¯¦ç»†ç»“æ„åŒ–æ•°æ®ï¼š</h4>')
                html.append('<pre>' + str(data['structured']) + '</pre>')
                html.append('</div>')

            html.append('</div>')

    html.append('''
        <hr>
        <div class="footer">
            <p>æœ¬æŠ¥å‘Šç”±ç»ˆææ™ºèƒ½çˆ¬è™«è‡ªåŠ¨ç”Ÿæˆ Â· ä»…ä¾›ä¸ªäººå­¦ä¹ ç ”ç©¶</p>
        </div>
    </div>
</body>
</html>''')

    return '\n'.join(html)

def scrape() -> None:
    logging.info("ç»ˆææ™ºèƒ½çˆ¬è™«å¼€å§‹è¿è¡Œ")

    os.makedirs(DATA_DIR, exist_ok=True)

    visited = load_set(VISITED_FILE)
    pending = load_list(PENDING_FILE)

    if not pending and os.path.exists(SEEDS_FILE):
        with open(SEEDS_FILE, 'r', encoding='utf-8') as f:
            seeds = [line.strip() for line in f if line.strip()]
        seeds = [s for s in seeds if is_allowed_domain(s)]
        pending = seeds
        logging.info(f"ä»ç§å­æ–‡ä»¶åŠ è½½äº† {len(seeds)} ä¸ªèµ·å§‹ç½‘å€")

    if not pending:
        logging.warning("æ²¡æœ‰å¾…æŠ“å–ç½‘å€ï¼Œè¯·æ£€æŸ¥ seeds.txt æˆ– pending.txt")
        return

    pages_crawled = 0
    new_links_found = 0
    failed_urls = []
    new_pending = []
    page_details = []

    while pending and pages_crawled < MAX_PAGES_PER_RUN:
        url = pending.pop(0)
        norm_url = normalize_url(url)

        if norm_url in visited:
            continue
        if not is_allowed_domain(url):
            logging.debug(f"è·³è¿‡ä¸å…è®¸çš„åŸŸå: {url}")
            continue
        if not can_fetch(url):
            logging.info(f"robots.txt ç¦æ­¢æŠ“å–: {url}")
            visited.add(norm_url)
            continue

        logging.info(f"æŠ“å– [{pages_crawled+1}/{MAX_PAGES_PER_RUN}]: {url}")

        try:
            headers = {'User-Agent': USER_AGENT}
            resp = requests.get(url, headers=headers, timeout=15)
            resp.raise_for_status()
            html = resp.text

            safe_fname = re.sub(r'[^\w\-_]', '_', url)[:150] + ".html"
            filepath = os.path.join(DATA_DIR, safe_fname)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(html)

            page_data = extract_page_data(html, url)
            page_details.append(page_data)

            soup = BeautifulSoup(html, 'lxml')
            links = extract_links(soup, url)
            for link in links:
                if link not in visited and link not in new_pending:
                    if is_allowed_domain(link):
                        new_pending.append(link)
                        new_links_found += 1

            visited.add(norm_url)
            pages_crawled += 1
            time.sleep(REQUEST_DELAY)

        except Exception as e:
            logging.error(f"æŠ“å–å¤±è´¥ {url}: {e}")
            failed_urls.append(url)

    all_pending = pending + new_pending
    seen = set()
    unique_pending = []
    for u in all_pending:
        nu = normalize_url(u)
        if nu not in seen and nu not in visited:
            seen.add(nu)
            unique_pending.append(u)

    save_set(VISITED_FILE, visited)
    save_list(PENDING_FILE, unique_pending)

    # ç”Ÿæˆ HTML æŠ¥å‘Š
    html_report = generate_html_report(
        pages_crawled=pages_crawled,
        new_links_found=new_links_found,
        failed_urls=failed_urls,
        unique_pending=unique_pending,
        visited_count=len(visited),
        page_details=page_details
    )

    logging.info("HTMLæŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼Œé•¿åº¦ï¼š%d å­—ç¬¦", len(html_report))

    if page_details:
        send_html_email(f"ç»ˆæçˆ¬è™«ç®€æŠ¥ - {pages_crawled} é¡µ", html_report)
    else:
        logging.info("æœ¬æ¬¡æ— æ–°å†…å®¹ï¼Œä¸å‘é€é‚®ä»¶")

if __name__ == "__main__":
    scrape()